from fastapi import FastAPI
from fastapi.responses import StreamingResponse
# Initializes the OpenAI client, allowing interaction with OpenAI's API.
from openai import OpenAI
from dotenv import load_dotenv, find_dotenv

_: bool = load_dotenv(find_dotenv())  # read local .env file

app = FastAPI()

# Initialize OpenAI client
client = OpenAI()


@app.get("/")
async def read_root():
    return {"response": "streaming openai with fastapi"}


@app.post("/dummy_stream")
# Dummy generator function to simulate streaming response
def dummy_event_stream():
    messages = [
        "This is part 1 of the response.\n",
        "This is part 2 of the response.\n",
        "This is part 3 of the response.\n",
    ]
    for message in messages:
        yield message
    return StreamingResponse(dummy_event_stream(), media_type="text/plain")
# Returns a StreamingResponse that streams the content generated by dummy_event_stream, with a MIME type of text/plain.


# Creates a streaming completion request to the OpenAI API using the specified model and message.
@app.post("/OpenAI_stream")
async def stream_openai():
    stream = client.chat.completions.create(
        model="gpt-3.5-turbo-1106",
        messages=[{"role": "user", "content": "Say this is a test"}],
        stream=True,
    )

    # Defines an inner generator function event_stream to generate parts of the response.
    def event_stream():
        for part in stream:
            # Extracts the content from the current part of the response, defaulting to an empty string if content is None.
            content = part.choices[0].delta.content or ""
            #If there is content, it yields the content, allowing it to be sent to the client incrementally.
            if content:
                yield content

#Returns a StreamingResponse that streams the content generated by event_stream, with a MIME type of text/plain.
    return StreamingResponse(event_stream(), media_type="text/plain")

